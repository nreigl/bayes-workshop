---
title: "Simple regression -- adding a predictor"
author: "Taavi Päll and Ülo Maiväli"
date: "2021-10-02"
output: github_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading some required libraries

```{r}
library(tidyverse)
library(here)
library(brms)
library(bayesplot)
library(tidybayes)
library(modelr)
```

### Simple regression -- adding a predictor

We will use **WaffleDivorce** data for the individual States of the USA, 
describing number of Waffle House diners and various marriage and 
demographic facts.

Loading WaffleDivorce data from rethinking package.

```{r}
(waffledivorce <- read_csv2(here("data/waffledivorce.csv")))
```

Where **Divorce** is 2009 divorce rate per 1000 adults and **Marriage** is 2009 marriage rate per 1000 adults.

> We want to see how Marriage rate predicts Divorce rate. 

In order to get divorced you need to get married first, but there is no reason 
to believe that high marriage rate causes high divorce rate.

There are at least two possible scenarios with high marriage rate, first, 
high rate means that marriage is highly valued in a society or, second, 
marriage is inflated.

First scenario may imply that marriage rate is negatively associated with 
divorce rate. Second scenario is compatible with positive association.

```{r}
waffledivorce %>% 
  ggplot(aes(x = Marriage, y = Divorce)) +
  geom_point() +
  geom_smooth(method = "lm") +
  coord_fixed() +
  labs(x = "Marriage rate", y = "Divorce rate")
```

Let's calculate summary stats from non-scaled data:
```{r}
waffledivorce %>% 
  mutate_at("MedianAgeMarriage", ~.x / 10) %>% 
  summarise_at(
    c("Marriage", "Divorce", "MedianAgeMarriage"), 
    list(median = median, sd = sd)
    )
```

For modeling, we want to bring marriage and divorce rate into relative scale, 
given that negative divorce rate does not make sense when marriage rate equals 
zero.

We are using `standardize` function from **rethinking** package as it returns vector 
in contrast to `scale` function. `standardize` is a wrapper around `scale` 
anyway and keeping matrices in our tibbles causes trouble for some downstream functions.

```{r}
divorce <- waffledivorce %>% 
  select(Loc, Marriage, Divorce, MedianAgeMarriage) %>% 
  mutate_at(c("Marriage", "Divorce", "MedianAgeMarriage"), rethinking::standardize)
```

#### Model

Here we specify our model by using priors from non-scaled original data.

$$D_i \sim \text{Normal}(\mu_i, \sigma)\quad\text{[likelihood]}$$
$$\mu_i = \alpha + \beta M_i\quad\text{[linear model]}$$
$$\alpha \sim \text{Normal}(97.5, 18.5)\quad[\alpha\ \text{prior}]$$
$$\beta \sim \text{Normal}(0, 2.5)\quad[\beta\ \text{prior}]$$
$$\sigma \sim \text{Normal}(0, 18.5)\quad[\sigma\ \text{prior}]$$

- Likelihood denotes that *i* on $\mu_i$ indicates that the mean depends upon the row. *D* denotes divorce rate.      
- The mean \mu is no longer a parameter to be estimated, $\mu_i$ is constructed from other parameters, $\alpha$ and $\beta$, and the predictor variable *M*, marriage rate. There is also deterministic relationship between $\mu$ and $\alpha$ and $\beta$, denoted by "=".   
- Then there are weakly informative priors for alpha, sigma and beta.    


We can see that by default, **brms** provides **flat** priors for beta coefficients,
so minimally we should provide these.
```{r}
f <- Divorce ~ Marriage
get_prior(formula = f, data = divorce, family = gaussian())
```

First, we can test if our prior choice is reasonable by sampling only from prior:

Exercise, please choose your (better than default) priors for Intercept, beta and sigma parameters 
using **scaled divorce** data and fit the model.

```{r}
priors <- c(
  prior("normal(0, 2.5)", class = "Intercept"),
  prior("normal(0, 1)", class = "b"),
  prior("normal(0, 2.5)", class = "sigma")
)
mod4.0 <- brm(
  formula = f,
  data = divorce,
  family = gaussian(),
  prior = priors,
  chains = 3,
  file = here("models/Divorce~Marriage_prior_only"),
  sample_prior = "only"
)
```


```{r}
summary(mod4.0)
```


Run pp_check multiple times, to get impression of prior-only performance:
```{r}
pp_check(mod4.0)
```

Let's have a look how well our prior-only model recovers different distribution parameters
```{r}
y <- as.numeric(divorce$Divorce) 
yrep <- posterior_predict(mod4.0)
```

We can check mean and max values in replications:
```{r}
ppc_stat(y, yrep, stat = "mean", binwidth = 1)
```

```{r}
ppc_stat(y, yrep, stat = "sd", binwidth = 1)
```

```{r}
ppc_stat(y, yrep, stat = "max", binwidth = 1)
```


Let's visualize (keeping only small amount of samples to avoid over-plotting):
```{r}
samples4.0 <- as_tibble(posterior_samples(mod4.0)) %>% 
  sample_n(50)
```

Here we can see a sample of plausible regression lines, based on our prior-only model:
```{r}
ggplot() +
  geom_point(data = divorce, aes(Marriage, Divorce)) +
  geom_abline(data = samples4.0, aes(slope = b_Marriage, intercept = b_Intercept), size = 0.3, alpha = 0.3) +
  labs(x = "Marriage rate", y = "Divorce rate") +
  theme_classic()
```



```{r}
priors <- c(
  prior("normal(0, 0.2)", class = "Intercept"),
  prior("normal(0, 0.05)", class = "b"),
  prior("normal(0, 1)", class = "sigma")
)
mod4.2 <- brm(
  formula = f,
  data = divorce,
  family = gaussian(),
  prior = priors,
  chains = 3,
  file = here("models/Divorce~Marriage"),
  sample_prior = "yes"
)
```


```{r}
summary(mod4.2)
```


```{r}
plot(mod4.2)
```

#### Visualize the model's inferences.


```{r}
samples4.2 <- as_tibble(posterior_samples(mod4.2)) %>% 
  sample_n(50)
```

We can see that, after including data to fit our model, region occupied by 
plausible regression lines becomes much more constrained:

```{r}
ggplot() +
  geom_point(data = divorce, aes(Marriage, Divorce)) +
  geom_abline(data = samples4.2, aes(slope = b_Marriage, intercept = b_Intercept), size = 0.3, alpha = 0.3) +
  labs(x = "Marriage rate", y = "Divorce rate") +
  theme_classic()
```


Let's add residuals and fitted values to our data:
```{r}
(post_sum4.2 <- divorce %>% 
   select(Marriage, Divorce) %>% 
   add_epred_draws(mod4.2) %>% 
   summarise_at(".epred", mean))
```

```{r}
coefs <- fixef(mod4.2)
post_sum4.2 %>% 
  ggplot(aes(x = Marriage)) +
  geom_point(aes(y = Divorce)) +
  geom_abline(slope = coefs[2, 1], intercept = coefs[1, 1]) +
  geom_segment(aes(xend = Marriage, y = .epred, yend = Divorce), size = 1/3, color = "blue")
```


#### Conditional effects

Model can be relatively easy visualized using `conditional_effects()` function, which displays 
conditional effects of one or more numeric and/or categorical predictors 
including two-way interaction effects.

Our simple model will look like so:
```{r}
plot(conditional_effects(mod4.2), points = TRUE, ask = FALSE)
```


#### Does marriage rate predict divorce rate?

We want to test one-sided hypothesis whether b_Marriage coefficient is bigger than 0.
```{r}
get_variables(mod4.2)
```


```{r}
(h4.2 <- hypothesis(mod4.2, "Marriage > 0"))
```

This model suggest that marriage rate does not predict divorce rate in US.
```{r}
plot(h4.2)
```


What happens if you use less stringent priors?


### Multiple regression -- adding another predictor

We know that probability of getting married and divorced grows with age. So let's 
add age at marriage to our model.

```{r}
divorce %>% 
  ggplot(aes(MedianAgeMarriage, Divorce)) +
  geom_point() +
  geom_smooth(method = "lm")
```


#### Model

To accommodate more variables into our model, we just expand the linear model by adding more coefficients.

$$D_i \sim \text{Normal}(\mu_i, \sigma)\quad\text{[likelihood]}$$

$$\mu_i = \alpha + \beta_M M_i + \beta_A A_i \quad\text{[linear model]}$$

$$\alpha \sim \text{Normal}(0, 0.2)\quad[\alpha\ \text{prior}]$$
$$\beta_M \sim \text{Normal}(0, 0.5)\quad[\beta_M\ \text{prior}]$$

$$\beta_A \sim \text{Normal}(0, 0.5)\quad[\beta_A\ \text{prior}]$$

$$\sigma \sim \text{Normal}(0, 1)\quad[\sigma\ \text{prior}]$$

- Now, $\beta_M$ and $\beta_A$ are coefficients for the predictor variable *M*, 
marriage rate, and predictor variable *A*, median age at marriage, respectively.
   

Here are our default priors:
```{r}
f <- Divorce ~ Marriage + MedianAgeMarriage
get_prior(formula = f, data = divorce, family = gaussian())
```

We can see that divorce rate decreases with increasing age at marriage.
```{r}
divorce %>% 
  ggplot(aes(MedianAgeMarriage, Divorce)) +
  geom_point() +
  geom_smooth(method = "lm")
```


```{r}
priors <- c(
  prior("normal(0, 0.2)", class = "Intercept"),
  prior("normal(0, 0.5)", class = "b"),
  prior("normal(0, 1)", class = "sigma")
)
mod5.0 <- brm(
  formula = f,
  data = divorce,
  family = gaussian(),
  prior = priors,
  chains = 3,
  file = here("models/Divorce~Marriage+MedianAgeMarriage"),
  sample_prior = "yes"
)
```


```{r}
summary(mod5.0)
```

```{r}
plot(mod5.0)
```



```{r}
pp_check(mod5.0)
```



