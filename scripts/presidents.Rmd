---
title: "Starting with Bayes statistics by estimating mean"
subtitle: "Simple intercept-only model"
author: "Taavi Päll and Ülo Maiväli"
date: "2021-10-02"
output: github_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading some required libraries

```{r}
library(tidyverse)
library(here)
library(brms)
library(bayesplot)
library(tidybayes)
```

## Getting data

We will use USA president heights data.

### Downloading US president heights data

President heights were copy-pasted from
[potus.com](https://www.potus.com/presidential-facts/presidential-heights/)
and after preprocessing (keeping only names and height in cm) saved to
`data` subfolder in our project folder.

### Importing president heights data


```{r}
(president_heights <- read_csv(here("data/president_heights.csv"), col_types = "dcd"))
```

We have two columns in our tibble -- presidents names ("name") and
height in cm ("height_cm").

## Visualization

As always, any analysis should start with data visualization to avoid
[Datasaurus](https://itsalocke.com/datasaurus/) appearing in the end.

Simple scatter plot, starting with tallest presidents.

-   Abraham Lincoln was the tallest president at 193 cm.
-   James Madison was the shortest president at 163 cm.
-   The average height of the presidents is 180 cm.

```{r}
ggplot(data = president_heights) +
 geom_point(aes(x = height_cm, y = fct_reorder(name, height_cm))) +
 theme(axis.title.y = element_blank())
```

Histogram shows that most frequently US presidents have been 183 cm
tall.

```{r}
ggplot(data = president_heights) +
 geom_histogram(aes(x = height_cm), binwidth = 1) +
 scale_y_continuous(breaks = scales::pretty_breaks())
```


Median and standard deviation of all presidents:
```{r}
president_heights %>% 
  summarise_at("height_cm", list(median = median, sd = sd))
```


## Modeling

### Simple intercept-only model

We denote our intercept-only model like so:

$$h_i \sim \text{Normal}(\mu, \sigma)$$
$$\mu \sim \text{Normal}(178, 20)$$
$$\sigma \sim \text{Normal}(7.4, 2)$$

As for R model formula, on left side we define "height_cm" as our
response variable (must be in data) and on the right side we define that
we are interested only in modeling "Intercept".


We keep only last ten presidents in our sample, as it's is well known that 
people have become taller during last 230 years.
```{r}
(president_heights <- president_heights %>% 
  arrange(number) %>% 
  tail(10))
```


> Here, for the sake of our demonstrations, we define that intercept is also part of all regression coefficients, so that sampling from its prior becomes possible, that's why use ... ~ 0 + Intercept + ... in the formulas.

```{r}
f <- height_cm ~ 0 + Intercept # height_cm ~ 1
```

Let's have a look at the parameters in our model for which we can
specify priors and default priors.

```{r}
get_prior(formula = f, data = data, family = gaussian())
```

To fit a **brms** model, we need to specify minimally:

1.  **model formula** in lme4 syntax
2.  **data** as data.frame and
3.  **family** to specify response distribution and **link function**.

Additionally, we want to run three chains and save fitted model to a
file in `models` subfolder (next line creates this folder if missing) to
avoid always refitting when updating and rerunning the script.

If you need to refit the model, then go to models folder and delete the
model file (.rds format).

```{r}
if (!dir.exists(here("models"))) dir.create(here("models")) # we keep models only locally
```

#### Testing different priors for Intercept

Here we fit intercept-only model using president heights data and
**uninformative** priors: "let data speak".

There are several reasons for using non-informative priors, including:

-   Not having any useful prior information or strong personal opinion
    upon which to base an informative prior.  
-   a non-informative prior gives a result numerically similar to a
    frequentist approach when little or no prior information is
    provided, while allowing for use of prior information when it
    exists.  
-   *ad hoc* sensitivity analysis to see how much influence a strong
    prior has had on the results of a Bayesian analysis.

```{r}
priors <- c(
  prior("normal(0, 200)", class = "b", coef = "Intercept"),
  prior("normal(7.4, 2)", class = "sigma")
  )
mod1 <- brm(
 formula = f, 
 data = president_heights, 
 family = gaussian(), 
 prior = priors,
 chains = 3, 
 file = here("models/height_cm~0+Intercept_informative_sigma"),
 sample_prior = "yes"
 )
```

#### Model diagnostics parameters

**Rhat** function produces R-hat convergence diagnostic, which compares
the between- and within-chain estimates for model parameters. If chains
have not mixed well (ie, the between- and within-chain estimates don't
agree), Rhat is larger than 1. It's recommend to run at least four
chains by default and only using the sample if Rhat is less than 1.05.

Both **bulk-ESS** and **tail-ESS** should be at least \~100 per Markov
Chain in order to be reliable and indicate that estimates of respective
posterior quantiles are reliable.

```{r}
summary(mod1)
```

```{r}
plot(mod1)
```

Let's check our priors from model:

```{r}
mod1$prior
```

```{r}
post_sam1 <- as_tibble(posterior_samples(mod1))
```

```{r}
post_sam1 %>% 
 select(matches("Intercept")) %>% 
 pivot_longer(cols = matches("Intercept")) %>% 
 ggplot() +
 geom_density(aes(value, linetype = name)) +
 labs(title = str_c("Uninformative prior: ", mod1$prior[2, 1])) +
 facet_wrap(~name, ncol = 1, scales = "free_y") +
  scale_x_continuous(limits = c(100, 200))
```


**Weakly informative prior**

To specifying weakly informative prior for Intercept, we can use
information obtained from Wikipedia, that average male height in USA is
178 cm (measured 2015-2018).

Weakly informative priors are designed to provide moderate
regularization and help stabilize computation.

```{r}
priors <- c(
  prior("normal(178, 20)", class = "b", coef = "Intercept"),
  prior("normal(7.4, 2)", class = "sigma")
  )
mod2 <- brm(
  formula = f, 
  data = president_heights, 
  family = gaussian(), 
  prior = priors, 
  chains = 3, 
  file = here("models/height_cm~0+Intercept_weakly_informative_sigma"), 
  sample_prior = "yes"
  )
summary(mod2)
```

```{r}
plot(mod2)
```

```{r}
post_sam2 <- as_tibble(posterior_samples(mod2))
post_sam2 %>% 
  select(matches("Intercept")) %>% 
  pivot_longer(cols = matches("Intercept")) %>% 
  ggplot() + 
  geom_density(aes(value, linetype = name)) + 
  labs(title = str_c("Weakly informative prior: ", mod2$prior[2,1])) + 
  facet_wrap(~name, ncol = 1, scales = "free_y")
```

**Bad informative prior**

Now, let's see what happens when our prior is well off, suppose we got
our prior from NBA.

```{r}
priors <- c(
  prior("normal(199, 1)", class = "b", coef = "Intercept"),
  prior("normal(7.4, 2)", class = "sigma"))
mod3 <- brm(
  formula = f, 
  data = president_heights, 
  family = gaussian(), 
  prior = priors, 
  chains = 3, 
  file = here("models/height_cm~1_bad_informative_prior_sigma"), 
  sample_prior = "yes" 
  )
summary(mod3) 
```

```{r}
plot(mod3)
```

```{r}
post_sam3 <- as_tibble(posterior_samples(mod3))
post_sam3 %>% 
  select(matches("Intercept")) %>% 
  pivot_longer(cols = matches("Intercept")) %>% 
  ggplot() + 
  geom_density(aes(value, linetype = name)) + 
  labs(title = str_c("Bad informative prior: ", mod3$prior[2,1])) +
  facet_wrap(~name, ncol = 1, scales = "free_y")
```

Compare last result with good/weakly informative priors (compare Intercept of all these models).

#### Inference from different priors

Let's compare, how much influence different priors have on inference.

We test hypothesis that USA presidents are above average tall, with
contemporary average male height 178 cm.

- Uninformative prior: "let data speak"

```{r}
plot(hypothesis(mod1, "Intercept > 178"), plot = FALSE)[[1]] + 
  labs(title = str_c("Uninformative prior: ", mod1$prior[2,1])) + 
  scale_x_continuous(limits = c(-50, 50))
```


- Weakly informative prior

```{r}
plot(hypothesis(mod2, "Intercept > 178"), plot = FALSE)[[1]] + 
  labs(title = str_c("Weakly informative prior: ", mod2$prior[2,1]))
```

- Bad informative prior

```{r}
plot(hypothesis(mod3, "Intercept > 178"), plot = FALSE)[[1]] + 
  labs(title = str_c("Bad informative prior: ", mod3$prior[2,1]))
```


#### Conclusion from using different priors

We can see that in our models, **posterior displays less variation than
prior**, that's because posterior distribution incorporates information
from data, we can expect that it's less variable than prior
distribution.

### Sampling from prior only

When setting up model, we can start with drawing samples solely from the priors 
ignoring the likelihood, which allows to generate samples from the prior
predictive distribution. 

```{r}
mod2.1 <- brm(
  formula = f, 
  data = president_heights, 
  family = gaussian(), 
  prior = prior("normal(178, 20)", class = "b", coef = "Intercept"), 
  chains = 3, 
  file = here("models/height_cm~0+Intercept_prior_only"), 
  sample_prior = "only"
  )
summary(mod2.1)
```

Generating posterior predictive samples from prior-only model. 
Chosen prior seems to generate reasonable values.
Black line denotes data, blue lines denote samples from prior-predictive 
distribution.

```{r}
set.seed(12) 
pp_check(mod2.1, nsamples = 10)
```

We can use plotting functions from *bayesplot* package to generate 
diagnostic plots for summary statistics of prior-predictive samples.

For this we need our data as *y* and samples drawn from model *yrep*.

```{r}
y <- president_heights$height_cm 
yrep <- posterior_predict(mod2.1)
```

We can check mean and max values in replications:
**Mean**
```{r}
ppc_stat(y, yrep, stat = "mean", binwidth = 5)
```

**Max**
```{r}
ppc_stat(y, yrep, stat = "max", binwidth = 5)
```

Or look at custom summary statistics, e.g. quantiles
25% quantile
```{r}
q25 <- function(x) quantile(x, 0.25) 
ppc_stat(y, yrep, stat = "q25", binwidth = 5)
```

75% quantile
```{r}
q75 <- function(x) quantile(x, 0.75)
ppc_stat(y, yrep, stat = "q75", binwidth = 5)
```

Plot central quantile posterior interval estimates
```{r}
mcmc_areas(mod2.1, pars = "b_Intercept", prob = 0.5, prob_outer = 0.9)
```


```{r}
posterior_summary(mod2.1, probs = c(0.05, 0.95))
```




### Getting draws

There are easier and foolproof methods getting posterior draws.

First, let's see what variables we have saved in our model:
```{r}
get_variables(mod2)
```

Extract desired draws from a Bayesian model:
```{r}
draws <- as_tibble(as.data.frame(mod2))
```


We can retest our hypothesis about US presidents:
```{r}
draws %>%
  summarise(h = mean((b_Intercept - 178) > 0), N = n())
```

```{r}
draws %>% 
  ggplot() +
  geom_density(aes(b_Intercept - 178)) +
  geom_vline(xintercept = 0, linetype = "dashed")
```


Calculate summary statistics:
```{r}
draws %>% 
  posterior_summary()
```


#### Pairs plot

We can see from draws tibble that draws from each iteration have tuple of b_Intercept and sigma. Let's see how they are related in our posterior.

Samples from the posterior distribution for the heights data. 
The density of points is highest in the center, reflecting the most plausible combinations of \mu and \sigma.
```{r}
draws %>% 
  ggplot(aes(sigma, b_Intercept)) +
  geom_point(alpha = 1/5) +
  coord_equal()
```


More elaborate plot with :
```{r}
pairs(mod2)
```

### Individual work 

Run models with different priors, using random sample N = 3 from presidents 
dataset.

Set seed for reproducibility!
```{r}
set.seed(20210920)
data <- sample_n(president_heights, 3)
```


### Simple regression -- adding a predictor

We will use WaffleDivorce data for the individual States of the USA, 
describing number of Waffle House diners and various marriage and 
demographic facts.

Loading WaffleDivorce data from rethinking package.
```{r}
(divorce <- read_csv2(here("data/waffledivorce.csv")))
```

Where **Divorce** is 2009 divorce rate per 1000 adults and **Marriage** is 2009 marriage rate per 1000 adults.

> We want to see how Marriage rate predicts Divorce rate.

```{r}
divorce %>% 
  ggplot() +
  geom_point(aes(x = Marriage, y = Divorce)) +
  coord_fixed()
```

Let's calculate some summary stats to plug into our priors:
```{r}
divorce %>% 
  summarise_at(
    c("Marriage", "Divorce"), 
    list(median = median, sd = sd)
    )
```



#### Model



$$h_i \sim \text{Normal}(\mu_i, \sigma)\quad\text{[likelihood]}$$
$$\mu_i = \alpha + \beta x_i\quad\text{[linear model]}$$
$$\alpha \sim \text{Student_t}(3, 97.5, 18.5)\quad[\alpha\ \text{prior}]$$
$$\beta \sim \text{Normal}(0, 5)\quad[\beta\ \text{prior}]$$
$$\sigma \sim \text{Student_t}(3, 0, 18.5)\quad[\sigma\ \text{prior}]$$

- Likelihood denotes that *i* on $\mu_i$ indicates that the mean depends upon the row.   
- The mean \mu is no longer a parameter to be estimated, $\mu_i$ is constructed from other parameters, $\alpha$ and $\beta$, and the predictor variable *x*. There is also deterministic relationship between $\mu$ and $\alpha$ and $\beta$, denoted by "=".   

- Then there are weakly informative priors for alpha, sigma and beta. 


We can see that by default, **brms** provides **flat** priors for beta coefficients,
so minimally we should provide these.
```{r}
f <- Divorce ~ Marriage
get_prior(formula = f, data = divorce, family = gaussian())
```

First, we can test if our prior choice is reasonable by sampling only from prior:
```{r}
mod4.0 <- brm(
  formula = f,
  data = divorce,
  family = gaussian(),
  prior = prior("normal(0, 2)", class = "b"),
  chains = 3,
  file = here("models/Divorce~Marriage_prior_only"),
  sample_prior = "only"
)
summary(mod4.0)
```

Run pp_check multiple times, to get good impression of draws:
```{r}
pp_check(mod4)
```

Let's have a look how well our prior-only model recovers different distribution parameters
```{r}
y <- divorce$Divorce 
yrep <- posterior_predict(mod4.0)
```

We can check mean and max values in replications:
```{r}
ppc_stat(y, yrep, stat = "mean", binwidth = 1)
```

```{r}
ppc_stat(y, yrep, stat = "sd", binwidth = 1)
```

```{r}
ppc_stat(y, yrep, stat = "max", binwidth = 1)
```


Lets test more priors:

```{r}
priors <- c(
  prior("normal(0, 2)", class = "b"),
  prior("normal(100, 10)", class = "Intercept"),
  prior("normal(0, 10)", class = "sigma")
  )
mod4.1 <- brm(
  formula = f,
  data = divorce,
  family = gaussian(),
  prior = priors,
  chains = 3,
  file = here("models/Divorce~Marriage_prior_only_normal"),
  sample_prior = "only"
)
summary(mod4.1)
```


```{r}
yrep <- posterior_predict(mod4.1)
```


```{r}
ppc_stat(y, yrep, stat = "mean", binwidth = 1)
```


```{r}
ppc_stat(y, yrep, stat = "sd", binwidth = 1)
```


```{r}
ppc_stat(y, yrep, stat = "max", binwidth = 1)
```


So are we going to use default priors or go on with normal priors?


